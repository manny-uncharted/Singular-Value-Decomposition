{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9q9JZjt16AMf"
   },
   "source": [
    "# Project - GAN\n",
    "\n",
    "Generative Adversarial Networks (GANs) were introduced by Ian Goodfellow, et al. in 2014. The ease of generating (usually images) using neural networks and backpropagation made it a quick success. GANs have already established themselves as a specialized field in Machine Learning. \n",
    "\n",
    "There have been a lot of variants of GANs (as of writing it, NVIDIA has launched StyleGANv3 as well). In this project, we will implement Deep Convolution (DC-GAN).\n",
    "\n",
    "![Generator architecture](dcgan-generator.png)\n",
    "\n",
    "DCGAN's Architecture (Generator only) from [Radford et al (2016)](https://arxiv.org/pdf/1511.06434v2.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSaA8Mif6srP"
   },
   "source": [
    "## 1. Imports\n",
    "\n",
    "Import JAX, [JAX NumPy](https://jax.readthedocs.io/en/latest/jax.numpy.html),\n",
    "Flax, ordinary NumPy, and TensorFlow Datasets (TFDS). Flax can use any\n",
    "data-loading pipeline and this example demonstrates how to utilize TFDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inJ9bV636dRx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0FW1DHa6cfH"
   },
   "source": [
    "## 2. Define Generator and Discriminator\n",
    "\n",
    "If you recall, a GAN consists of two networks: Generator and Discriminator - both competing against each other in a min-max game.\n",
    "\n",
    "$$min_G max_D V(D,G) = {\\mathop{\\mathbb{E}}}_{x\\sim p_{data}(x)}[log D(x)]+\\mathbb E_{z\\sim p_{z}(z)}[log(1-D(G(z))]$$\n",
    "\n",
    "Creating a convolutional neural network is pretty straightforward with the Linen API by subclassing\n",
    "[`Module`](https://flax.readthedocs.io/en/latest/flax.linen.html#core-module-abstraction).\n",
    "Because the architecture in this example is relatively simple—you're just\n",
    "stacking layers—you can define the inlined submodules directly within the\n",
    "`__call__` method and wrap it with the\n",
    "[`@compact`](https://flax.readthedocs.io/en/latest/flax.linen.html#compact-methods)\n",
    "decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_s1lXBBO66dc"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    training: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, z):\n",
    "        x = nn.ConvTranspose(features=64*8, kernel_size=(4, 4),\n",
    "                             strides=(1, 1), padding='VALID', use_bias=False)(z)\n",
    "        x = nn.BatchNorm(\n",
    "            use_running_average=not self.training, momentum=0.9)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        #Repeat the block and remember this block should have half features. Rest is same.\n",
    "\n",
    "        \"\"\"Block/Layer 2 here\"\"\"\n",
    "\n",
    "        #Similarly half the features from previous block\n",
    "        \n",
    "        \"\"\"Block/layer 3 here\"\"\"\n",
    "\n",
    "        x = nn.ConvTranspose(features=1, kernel_size=(\n",
    "            4, 4), strides=(1, 1), padding='SAME', use_bias=False)(x)\n",
    "        return jnp.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    training: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Conv(features=64, kernel_size=(\n",
    "            3, 3), strides=(2, 2), padding='SAME', use_bias=False)(x)\n",
    "        x = nn.leaky_relu(x, negative_slope=0.2)\n",
    "        \n",
    "        #Repeat the block and remember this block should have double features. Rest is same.\n",
    "\n",
    "        \"\"\"Block/Layer 2 here\"\"\"\n",
    "\n",
    "        #Similarly double the features from previous block\n",
    "        \n",
    "        \"\"\"Block/layer 3 here\"\"\"\n",
    "\n",
    "\n",
    "        x = nn.Conv(features=1, kernel_size=(\n",
    "            1, 1), strides=(4, 4), padding='VALID', use_bias=False)(x)\n",
    "        x = jnp.reshape(x, [x.shape[0], -1])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDEoAprU6_JZ"
   },
   "source": [
    "## 3. Define loss\n",
    "\n",
    "Recall that GAN uses a binary cross-entropy loss function. This BCE loss will be further used to define the Generator and Discriminator losses respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zcb_ebU87G7s"
   },
   "outputs": [],
   "source": [
    "@jax.vmap\n",
    "def BCELoss(logit, label):\n",
    "    return \"\"\"Define BCE loss formula there\"\"\"\n",
    "\n",
    "\n",
    "def GeneratorLoss(generatorParams, discriminatorParams, batch, key, generatorVariables, discriminatorVariables):\n",
    "    z = \"\"\"Sample latent vector of 100 length using Normal/Gaussian distribution\"\"\"\n",
    "    \n",
    "    #Hints\n",
    "    #Make PyTree for both generator and discriminator Variables \n",
    "    #Return the mean of BCE Loss for Generator+Discriminator\n",
    "    return jnp.mean(BCELoss(\"\"\"add the respective variables here\"\"\")\n",
    "\n",
    "\n",
    "def DiscriminatorLoss(discriminatorParams, generatorParams, batch, key, generatorVariables, discriminatorVariables):\n",
    "    z = \"\"\"Sample latent vector of 100 length using Normal/Gaussian distribution\"\"\"\n",
    "\n",
    "    return \"\"\"Return consolidated loss by combining both real and fake losses\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYz0Emry-ele"
   },
   "source": [
    "## 4. Loading data\n",
    "\n",
    "For ease of use, this function is pre-implemented. MNIST is part of TF-Datasets (hence we imported TF and TF-DS at the start) and can be directly imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOeWiS_b-p8O"
   },
   "outputs": [],
   "source": [
    "def make_dataset(batch_size, seed=1):\n",
    "    mnist = tfds.load(\"mnist\")\n",
    "\n",
    "    def _preprocess(sample):\n",
    "        image = tf.image.convert_image_dtype(sample[\"image\"], tf.float32)\n",
    "        image = tf.image.resize(image, (32, 32))\n",
    "        return 2.0 * image - 1.0\n",
    "\n",
    "    ds = mnist[\"train\"]\n",
    "    ds = ds.map(map_func=_preprocess,\n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.shuffle(10 * batch_size, seed=seed).repeat().batch(batch_size)\n",
    "    return iter(tfds.as_numpy(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7l-75YE-sr-"
   },
   "source": [
    "## 5. Training step\n",
    "\n",
    "Now we will define the training (step-by-step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ng11cdMf-z0x"
   },
   "outputs": [],
   "source": [
    "@partial(jax.pmap, axis_name='batch')\n",
    "def TrainingStep(key, generatorVariables, discriminatorVariables, generatorOptimizer, discriminatorOptimizer, batch):\n",
    "    \n",
    "    \"\"\"your code here\"\"\"\n",
    "\n",
    "    return key, generatorVariables, discriminatorVariables, generatorOptimizer, discriminatorOptimizer, discriminatorLoss, generatorLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consolidate it all by training the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Now consolidate it all\"\"\"\n",
    "    return generatorLosses, discriminatorLosses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generatorLosses, discriminatorLosses = main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "annotated_mnist",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
